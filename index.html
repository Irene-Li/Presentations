<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title> Matrix Product State Inspired Tensor Networks </title>

		<meta name="author" content="Irene Li">
		
		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">


		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/ssolarized.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<h2> Matrix Product State Inspired Tensor Train </h2>
					<h3> [ Using Quantum Mechanics for Machine Learning ] </h3>
					<p> Irene Li, Taketomo Isazawa </p>
					<p> Supervisor: Dr Austen Lamacraft </p>

				<aside class="notes">
				I will be talking about a class of novel machine learning model called Tensor Train and its relation to quantum mechanics. 
				</aside>
				</section>

				<section> 
					<h2> Bibliography </h2>
					<ol>
						<li> Stoudenmire, E. Miles, and David J. Schwab. "Supervised learning with quantum-inspired tensor networks." (2016) </li>
						<li> Novikov, Alexander, Mikhail Trofimov, and Ivan Oseledets. "Exponential machines." (2016) </li>
						<li> Schollwöck, Ulrich. "The density-matrix renormalization group in the age of matrix product states." Annals of Physics 326.1 (2011): 96-192. </li>
						<li> Orús, Román. "A practical introduction to tensor networks: Matrix product states and projected entangled pair states." Annals of Physics 349 (2014): 117-158. </li>
					</ol>
				</section>

				<section> 
					<section data-markdown>	
						<textarea data-template>

							### The Story of Thomas the Tensor Train
							
							
									     _____                            . . . . o o o o o 
									   __|[_]|__ ___________ _______    ____      o
									   |[] [] []| [] [] [] [] [_____(__  ][]]_n_n__][.
									   _|________|_[_________]_[________]_|__|________)<
									   oo    oo 'oo      oo ' oo    oo 'oo 0000---oo\_
									 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

						</textarea>	
					</section>

					<section> 
						<h2> Machine Learning $\rightarrow$ Quantum Physics </h2>
						<p class="fragment"> <img height="60" data-src="images/sixsixsix.png" align="bottom" alt="6"> </p>
						<p class="fragment"> Each pixel lives in its vector space (spin) <br> 
							<img class="center" height="150" data-src="images/spin.png" alt="spins"> 
							</p>
						<p class="fragment"> But need $\mathcal{O}\left(2^{N_{\text{pixels}}}\right)$ parameters! </p>

					</section>

					<section> 
						<h2> Graphical Tensor Notation </h2>
						<p> Vector, matrix and tensor
						<br> <img class="center" height="150" data-src="images/graph_tensor.png" alt="tensor"> </p>
						<p> Tensor contraction <br> 
						<img class="center" height="150" data-src="images/tensor_contract.png" alt="contraction"> </p>
					</section>


					<section>
						<h2> Matrix Product State (MPS) </h2>
						<ul> 
							<li class="fragment"> A quantum many-body system 
								$$ 
								\lvert \psi \rangle = \sum_{{s_1}{s_2}{s_3}\dots{s_n}}W_{{s_1}{s_2}{s_3}\dots{s_n}} \lvert {{s_1}{s_2}{s_3}\dots{s_n}} \rangle
								$$
							</li>
							<li class="fragment"> As a matrix product state
								$$
								W_{{s_1}{s_2}{s_3}\dots{s_n}} \approx M^{(1)}_{{s_1}{\sigma_1}} 
															M^{(2)}_{{s_2}{\sigma_1}{\sigma_2}} 
															M^{(3)}_{{s_3}{\sigma_2}{\sigma_3}}
															\dots 
															M^{(n-1)}_{{s_{n-1}{\sigma_{n-1}}{\sigma_{n}}}}M^{(n)}_{{s_n}{\sigma_n}}
								$$ 
							</li>
							<li class="fragment"> Graphical tensor notation 
								<img class="center" height="80" data-src="images/mps.png" alt="mps">
								$$ 2^{N_{\text{pixels}}}  \text{ vs. }  2N_{\text{pixels}}d^2 $$ 
							</li>
						</ul>
					</section>

					<section>
						<h2> Amplitudes and probabilities </h2>
						<ul> 
							<li class="fragment"> In quantum mechanics
								<br>
								$$ P(\phi \vert \psi) = \vert \langle \phi \vert \psi \rangle \vert ^2 $$ 
							</li>
							<li class="fragment"> Require $\int P(\phi(u) \vert \psi) \mathrm{d}u = 1 $ for normalised $\lvert \psi \rangle$ </li>
							<li class="fragment"> E.g. overlap amplitude between a product state and an MPS <br>
									<img class="center" height="100" data-src="images/mps_with_inputs.png" alt="mps with inputs">

							</li> 
						</ul>
					</section>

					<section> 
						<h2> Modified MPS for Supervised Classification </h2> 
						<ul>
							<li class="fragment"> 
							Lay the input data along the chain <br>
							<img class="center"
								 height="100" 
								 data-src="images/classification.png" 
								 alt="classification">

							$$ f^l = W^l_{{s_1}{s_2}{s_3}\dots{s_n}} x^{(1)}_{s_1} x^{(2)}_{s_2} x^{(3)}_{s_3}
																		\dots x^{(n)}_{s_n} 
							$$
							Here we represent the output as one-hot vectors $[0, 0, \dots 1, \dots 0]$
							</li>
							<li class="fragment"> 
							Represent $W$ by a modified MPS 
							<br>
							<img class="center"
								 height="100"
							 	 data-src="images/classification_approx.png"
							 	 alt="classification approximation">								
							</li>
						</ul>
						<aside class="notes">
							- MPS is linear along each tensor leg -> having input as a long vector not useful!  <br> 
							- One hot vector: one for the right category and zero otherwise <br>
							- Can think of the output and the input collectively as some product state <br> 
							- In some sense we are calculating the overlap between that product state and the MPS <br> 
					</section>
				</section>	
				<section>
					<section>
						<h2> Preprocessing the input data </h2>
						<ul> 
							<li> Recall <a href="#/2/4"> the normalisation requirement </a>
							<li class="fragment"> Categorical data (e.g. character) $\rightarrow$ one-hot vector </li>
							<li class="fragment"> Real number in $[0, 1]$ (e.g. MNIST handwritten digits) 
								<ul> 
									<li> Option 1: $\left[\cos(\pi x/2), \sin(\pi x/2)\right]$ </li>
									<li> Option 2: $\left[1, \sqrt{3} (2x -1) \right]$ $\bigstar$  </li>
									<li> Option 3: $\left[\exp(i3\pi x/2)\cos(\pi x/2), \exp(-i3\pi x/2)\sin(\pi x/2)\right]$ </li>
								</ul>
								</li>
							<li class="fragment"> Real number ? </li>
						</ul>
						<aside class="notes"> 
						- Categorical data, essentially binary data, properly normalised <br> 
						- This is what is done in a paper by SS, not probabilistic, due to the nature of the input data (think about how the overlap changes as you rotate one by $\pi$) <br> 
						- Good for taking advantage of linear regression, and properly normalised! This is the one use <br> 
						- Normalised and look more like a spin. But it also forces us to use complex parameters <br>
						- For real number not limited to a range, need activation functions! But this is quite uncommon anyway.. 
						</aside>
					</section>
				</section>
				<section> 
					<section>
						<h2> How to train a tensor train? </h2>
						<ul> 
							<li class="fragment"> Minimise the loss function 
								$ L(\mathbf{f}_{\text{data}}, \mathbf{f}_{\text{guess}}) $ <br> 
							</li>
							<li class="fragment"> Training algorithms 
							<ul>
								<li> Stochastic Gradient Descent </li>
								<li> Density Matrix Renormalisation Group (DMRG) <br> 
										(Single-site and two-site)
									</li>
								<li> Riemannian Optimisation </li>
								<li> Any combination of the above </li>
							</ul>
							</li>
						</li>
						<aside class="notes">
							- SGD: take element-wise gradient and apply the change <br> 
							- DMRG: I am not going to spend anytime explaining what the words mean. We will talk about this in details next. It comes in two favours: one site DMRG takes gradient wrt one matrix whereas two site DMRG looks at two matrices at a time<br> 
							- It takes gradient wrt the whole MPS and then add the gradient on directly with some rounding so the size doesn't grow. <br> 
						</aside>
					</section>
					<section> 
						<h2> Aside on essential tools </h2>
						<ul> 
						<li class="fragment"> Singular Value Decomposition (SVD) 
							$$ M_{ij} = \sum_{k} U_{ik} S_{k} V_{kj} $$ 
							$ M \in \mathbb{R}^{m \times n}$, $U\in \mathbb{R}^{m \times m}$, 
							$S\in \mathbb{R}^{\text{min}(m, n)}$, $V\in \mathbb{R}^{n \times n} $ </li>
						<li class="fragment"> Index grouping (matricization) 
							<img class="center" height="220" data-src="images/grouping.png" alt="grouping" > </li>
						</ul>
					</section>

					<section> 
						<h2> Two-site DMRG </h2>
						<p> <img class="center" height="500" data-src="images/drmg1.png" alt="drmg1" ></p>
					</section>

					<section> 
						<h2> Two-site DMRG </h2>
						<p> <img class="center" height="500" data-src="images/drmg2.png" alt="drmg2" ></p>
					</section>

					<section> 
						<h2> Two-site DMRG pros and cons </h2>

						<ul> 
						<li class="fragment"> Pros 
							<ul> 
							<li> Analytic form for gradient </li>
							<li> Fast convergence </li>
							<li> Can capture long range correlations </li>
							<li> Light-weight model </li>
							<li> cost = $\mathcal{O}(d_{in}^3 m^3 d_{out} N_{\text{pixels}} N_{\text{images}})$ </li>
							</ul> 
						</li>
						<li class="fragment"> Cons 
							<ul> 
							<li> cost = $\mathcal{O}(d_{in}^3 m^3 d_{out} N_{\text{pixels}} N_{\text{images}})$ </li>
							<li> Tricks for conventional neural nets not applicable <br> (e.g. momentum) </li>
							<ul> 
						</li>


					<aside class="notes">
						- As opposed to most other Machine learning algorithms <br> 
						- Good convergence with a few thousand MNIST samples <br> 
						- Permuted data perform as well as unpermuted (very rare among machine learning algorithms!) <br> 
					</aside>

					</section>
				</section>

				<section> 
					<h2> Tricks for training </h2>
					<ul> 
					<li> Linear regression initialisation </li>
					<li> Use Newton's method $g/h$ </li>
					<li> Finetune the learning rate dynamically </li>
					<li> Use cross entropy as the loss function </li>
					</ul> 

					<aside class="notes">
						- For MNIST lin reg initialisation puts the accuracy at 85% to start with <br> 
						- Approximation the hessian matrix by its diagonal elements. This is taking the advantage of the fact that we know the analytic form of gradient. Slow but improves convergence <br>
						- Use the Armijo condition (draw on the board if there's time) <br> 
						- The alternative is the quadratic cost - more inline with the quantum point of view. But at least from our experience cross entropy seems to perform better <br> 
						- Randomly zeroing input data <br> 
					</aside>
				</section>

				<section> 
					<h2> Results for MNIST </h2>
					<table>
						<thead>
							<tr>
								<th>  </th>
								<th>Init</th>
								<th>Max matrix dim</th>
								<th>Accuracy</th>
							</tr>
						</thead>
						<tbody>
							<tr>
								<td> Lin Reg</td>
								<td> N/A </td>
								<td> N/A </td>
								<td> $85\%$</td>
							</tr>
							<tr>
								<td> 2-site DMRG </td>
								<td> None </td>
								<td> 20 </td>
								<td> $90\%$ </td>
							</tr>
							<tr> 
								<td> 2-site DMRG </td>
								<td> Linear </td>
								<td> 20 </td>
								<td> $90\%$ </td> 
							</tr>
							<tr> 
								<td> 2-site DMRG </td>
								<td> Linear </td>
								<td> 60 </td>
								<td> $96\%$</td> 
							</tr>
							<tr> 
								<td> 2-site DMRG (permuted) </td>
								<td> Linear </td>
								<td> 50 </td>
								<td> $96\%$</td>
							</tr>
							<tr> 
								<td> Stoudenmire $\&$ Schwab </td>
								<td> None </td>
								<td> 20 </td>
								<td> $99\%$ </td>
							</tr>
							<tr>
						</tbody>
					</table>
				</section>

				<section> 
					<section> 
						<h2> Sampling with MPS </h2>
						<p> Generate MNIST-like data given label </p>
						<ul> 
						<li class="fragment"> $P(x_1 x_2 \dots x_n) = P(x_1) P(x_2\vert x_1)
						\dots P(x_n \vert x_1 x_2 \dots x_{n-1}) $</li>
						<li class="fragment"> Sampling $\longleftrightarrow$ unzipping <br> 
						<img class="center" height="150" data-src="images/MPSchain1.png" alt="unzip"> 
						<img class="center" height="150" data-src="images/MPSchain2.png" alt="unzip"> 
						</li>
						</ul>
					<aside class="notes"> 
						Draw on the board to show how to sample from MPS 
					</aside>
					</section>
				</section>
				<section> 
					<h2> Prospects </h2>
					<ul> 
					<li> Try on more 1d datasets </li>
					<li> Enforcing periodicity </li>
					<li> Extend to variable length sequences </li>
					</ul>
				<aside class="notes"> 
				- mention infinite DMRG is a thing 
				</aside>
				</section>
				<section> 
					<h2> Thank you! </h2>
					<p> See our tensorflow implementation at <br> 
					 <a hrep = "https://github.com/TrMPS/MPS-MNIST"> https://github.com/TrMPS/MPS-MNIST </a> </p>
				</section>

				<section>
					<section> 
						<h2> The rejects. </h2>
					</section>
					<section> 
						<h2> Canonical form </h2>
						<ul> 
							<li class="fragment"> Norm 
								<img class="center" height="100" data-src="images/contraction.png" alt="overlap">		
							</li>
							<li class="fragment"> Left and right normalised matrices
								<img class="center" height="100" data-src='images/canonical.png' alt="canonical form">
								$$ U_{(s_j \sigma) \rho} U_{(s_j \sigma) \gamma} = \delta_{\rho \gamma} 
								\: \text{ and } \: V_{s_j \rho \sigma} V_{s_j \gamma \sigma} = \delta_{\rho \gamma} $$
								<br> </li>
							<li class="fragment"> Use singular value decomposition (svd) : $ M = USV $
								<img class="center" height="50" data-src="images/canonical_chain.png" alt='canonical chain'>
							</li>
						</ul>
						<aside class="notes">
							- Calculating the norm is like zipping two MPSs <br>
							- alt-click "left" to zoom in <br>
							- Treating some indices as collective indices, then U and V are left/right-orthogonal <br>
							- SVD is like diagonalisation for rectangular matrices <br>
							- SVD minimises L2 distance between two matrices 
							- Draw on the board to explain how to use svd to bring a chain into canonical form 
						</aside>
					</section>



					<section> 
						<h2> The Rounding Method </h2>
						<p> <img class="center" height="300" data-src="images/rounding.png" alt="rounding method"> </p>
						<p class="fragment"> Better cost but dimensions of all matrices are kept the same </p>
					</section>

				</section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				controls: true,
				progress: true, 
				history: true,
				overview: true,
				loop: false,
				center: true, 
				
				fragments: true,
				keyboard: true,
				transition: 'slide',

				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/math/math.js', async: true }
				]
			});
		</script>
	</body>
</html>
